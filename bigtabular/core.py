# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% ../nbs/00_core.ipynb 2
from __future__ import annotations
from fastai.torch_basics import *
from fastai.data.all import *
from fastai.tabular.all import *
import dask.dataframe as dd

# %% auto 0
__all__ = ['dask_make_date', 'dask_add_datepart', 'dask_add_elapsed_times', 'dask_cont_cat_split', 'get_random_train_mask',
           'TabularDask', 'DaskCategoryMap', 'DaskCategorify', 'DaskNormalize', 'DaskCategorize', 'DaskFillStrategy',
           'DaskFillMissing', 'DaskRegressionSetup', 'DaskCategoryBlock', 'DaskRegressionBlock', 'DaskDataLoader']

# %% ../nbs/00_core.ipynb 7
def dask_make_date(ddf, date_field):
    "Convert `df[date_field]` to date type."
    ddf[date_field] = dd.to_datetime(ddf[date_field], infer_datetime_format=True)

# %% ../nbs/00_core.ipynb 9
def dask_add_datepart(ddf, field_name, prefix=None, drop=True, time=False):
    "Helper function that adds columns relevant to a date in the column field_name of ddf"
    dask_make_date(ddf, field_name)
    # return ddf.map_partitions(lambda df: add_datepart(df, field_name, prefix=prefix, drop=drop, time=time))
    return ddf.map_partitions(partial(add_datepart, field_name=field_name, prefix=prefix, drop=drop, time=time))

# %% ../nbs/00_core.ipynb 15
def dask_add_elapsed_times(ddf, field_names, date_field, base_field):
    dask_make_date(ddf, date_field)
    # return ddf.map_partitions(lambda df: add_elapsed_times(df, field_names, date_field, base_field))
    return ddf.map_partitions(partial(add_elapsed_times, field_names=field_names, date_field=date_field, base_field=base_field))

# %% ../nbs/00_core.ipynb 17
def dask_cont_cat_split(df, max_card=20, dep_var=None):
    "Helper function that returns column names of cont and cat variables from given `df`."
    cont_names, cat_names = [], []
    for label in df:
        if label in L(dep_var): continue
        if ((pd.api.types.is_integer_dtype(df[label].dtype) and
            # Change for Dask compatibility
            df[label].nunique().compute() > max_card) or
            pd.api.types.is_float_dtype(df[label].dtype)):
            cont_names.append(label)
        else: cat_names.append(label)
    return cont_names, cat_names

# %% ../nbs/00_core.ipynb 25
def get_random_train_mask(df, train_frac=0.8):
    return pd.Series(np.random.random(len(df)) < train_frac)

# %% ../nbs/00_core.ipynb 28
# TODO: align this function with the the tabular.core version
class _TabIloc:
    "Get/set rows by iloc and cols by name"
    def __init__(self,to): self.to = to
    def __getitem__(self, idxs):
        df = self.to.items
        if isinstance(idxs,tuple):
            rows,cols = idxs
            cols = df.columns.isin(cols) if is_listy(cols) else df.columns.get_loc(cols)
        else: rows,cols = idxs,slice(None)
        return df.iloc[rows, cols]

# %% ../nbs/00_core.ipynb 29
class TabularDask(CollBase, GetAttr, IterableDataset):
    """
    A Dask `DataFrame` wrapper that knows which cols are cont/cat/y, and returns rows in `__iter__`.
    The aim is to replicate the TabularPandas API as closely as possible.
    """
    _default,with_cont='procs',True
    def __init__(
        self, ddf, procs=None, cat_names=None, cont_names=None, y_names=None, y_block=None, train_mask_func=None,
        do_setup=True, device=None, reset_index=True
    ):
        self.items = ddf.copy()
        # if "_int_train_mask" not in ddf.columns:
        #     if train_mask_func is None:
        #         train_mask_func = get_random_train_mask
        #     self.items["_int_train_mask"] = ddf.map_partitions(
        #         train_mask_func, meta=pd.Series(name="_int_train_mask", dtype="bool")
        #     )
        if "_int_train_mask" not in ddf.columns:
            if train_mask_func is None:
                self.items["_int_train_mask"] = True
            else:
                self.items["_int_train_mask"] = ddf.map_partitions(
                    train_mask_func, meta=pd.Series(name="_int_train_mask", dtype="bool")
                )
        if reset_index: ddf = ddf.reset_index(drop=True)
        # self._dl_type, self._dbunch_type = DaskDataLoader, DaskDataLoaders
        self.y_names, self.device = L(y_names), device

        if y_block is None and self.y_names:
            # Make ys categorical if they're not numeric
            ys = self.items[self.y_names]
            if len(ys.select_dtypes(include='number').columns)!=len(ys.columns):
                y_block = DaskCategoryBlock()
            else:
                y_block = DaskRegressionBlock()
        if y_block is not None and do_setup:
            if callable(y_block): y_block = y_block()
            # A bit hacky, but ensuring compatibility with `CategoryBlock` and `RegressionBlock`
            # TODO: don't think we need this anymore
            if isinstance(y_block.type_tfms[0], Categorize): y_block.type_tfms = DaskCategorize()
            elif isinstance(y_block.type_tfms[0], RegressionSetup): y_block.type_tfms = DaskRegressionSetup()
            procs = L(procs) + y_block.type_tfms

        self.cat_names, self.cont_names, self.procs = L(cat_names), L(cont_names), Pipeline(procs)
        self.start, self.end = 0, len(self.items)
        if do_setup: self.setup()

    def new(self, df):
        return type(self)(df, do_setup=False, y_block=TransformBlock(),
                          **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device'))
        
    def subset(self, i):
        train = self.items['_int_train_mask']
        return self.new(self.items[train if i==0 else ~train])

    def copy(self): self.items = self.items.copy(); return self
    def decode(self): return self.procs.decode(self)
    def decode_row(self, row):
        row = row.to_frame().T
        row[list(self.cont_names)] = row[list(self.cont_names)].astype(np.float32)
        row[list(self.cat_names)] = row[list(self.cat_names)].astype(np.int32)
        return self.new(dd.from_pandas(row)).decode().items.compute().iloc[0]

    def show(self, max_n=10, **kwargs):
        display_df(
            self.new(self.all_cols).decode().items.head(max_n).drop(columns="_int_train_mask")
        )

    def setup(self): self.procs.setup(self)
    def process(self): self.procs(self)
    def loc(self): return self.items.loc
    def iloc(self): return _TabIloc(self)
    def targ(self): return self.items[list(self.y_names)]
    def x_names (self): return self.cat_names + self.cont_names
    def n_subsets(self): return 2
    def y(self): return self[self.y_names[0]]
    def new_empty(self): raise NotImplementedError
    def to_device(self, d=None):
        self.device = d
        return self

    def all_col_names(self):
        ys = [n for n in self.y_names if n in self.items.columns]
        return self.x_names + self.y_names if len(ys) == len(self.y_names) else self.x_names

    def n_inp(self): return int(len(self.cat_names)>0) + int(len(self.cont_names)>0)
        
    def __iter__(self):
        cat_stop = len(self.cat_names)
        con_stop = cat_stop + len(self.cont_names)
        for i in range(self.items.npartitions):
            # df = self.items.get_partition(i).compute()[self.cat_names + self.cont_names + self.y_names]
            df = self.items.get_partition(i).compute()[self.all_col_names]
            ys = [n for n in self.y_names if n in self.items.columns]
            for row in df.itertuples(index=False):
                res = (list(row[:cat_stop]), list(row[cat_stop:con_stop]))
                if len(ys) == len(self.y_names): res = res + (list(row[con_stop:]),) 
                # yield (list(cats), list(conts), list(targ))
                yield res

    def transform(self, cols, f, all_col=True):
        if not all_col: cols = [c for c in cols if c in self.items.columns]
        if len(cols) > 0:
            meta_dtype = "int16" if cols[0] in self.cat_names else "float32"
            meta = pd.DataFrame({c: [] for c in cols}, dtype=meta_dtype)
            self[cols] = self[cols].map_partitions(lambda df: df.transform(f), meta=meta)

    def dataloaders(self, 
        bs:int=64, # Batch size
        shuffle_train:bool=None, # (Deprecated, use `shuffle`) Shuffle training `DataLoader`
        shuffle:bool=True, # Shuffle is currently ignored in `DaskDataLoader`
        val_shuffle:bool=False, # Shuffle validation `DataLoader`
        n:int=None, # Size of `Datasets` used to create `DataLoader`
        path:str|Path='.', # Path to put in `DataLoaders`
        dl_type:DataLoader=None, # Type of `DataLoader`
        dl_kwargs:list=None, # List of kwargs to pass to individual `DataLoader`s
        device:torch.device=None, # Device to put `DataLoaders`
        drop_last:bool=None, # Drop last incomplete batch, defaults to `shuffle`. Currently ignored in `DaskDataLoader`
        val_bs:int=None, # Validation batch size, defaults to `bs`
        **kwargs
    ) -> DataLoaders:
        if shuffle_train is not None:
            shuffle=shuffle_train
            warnings.warn('`shuffle_train` is deprecated. Use `shuffle` instead.',DeprecationWarning)
        if device is None: device=default_device()
        if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets
        if dl_type is None: dl_type = self._dl_type
        # if drop_last is None: drop_last = shuffle
        if shuffle or drop_last:
            shuffle, drop_last = False, False
            warnings.warn('`shuffle` and `drop_last` are currently ignored.')
        val_kwargs={k[4:]:v for k,v in kwargs.items() if k.startswith('val_')}
        def_kwargs = {'bs':bs,'shuffle':shuffle,'drop_last':drop_last,'n':n,'device':device}
        dl = dl_type(self.subset(0), **merge(kwargs,def_kwargs, dl_kwargs[0]))
        def_kwargs = {'bs':bs if val_bs is None else val_bs,'shuffle':val_shuffle,'n':None,'drop_last':False}
        dls = [dl] + [dl.new(self.subset(i), **merge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))
                      for i in range(1, self.n_subsets)]
        return self._dbunch_type(*dls, path=path, device=device)

properties(TabularDask,'iloc','targ','all_col_names','n_subsets','x_names','y', 'n_inp')

# %% ../nbs/00_core.ipynb 31
def _add_prop(cls, nm):
    @property
    def f(o): return o[list(getattr(o,nm+'_names'))]
    @f.setter
    def fset(o, v): o[getattr(o,nm+'_names')] = v
    setattr(cls, nm+'s', f)
    setattr(cls, nm+'s', fset)

_add_prop(TabularDask, 'cat')
_add_prop(TabularDask, 'cont')
_add_prop(TabularDask, 'y')
_add_prop(TabularDask, 'x')
_add_prop(TabularDask, 'all_col')

# %% ../nbs/00_core.ipynb 32
TabularDask.train, TabularDask.valid = add_props(lambda i,x: x.subset(i))

# %% ../nbs/00_core.ipynb 38
class DaskCategoryMap(CategoryMap):
    "Dask implementation of CategoryMap. Collection of categories with the reverse mapping in `o2i`"
    def __init__(self, col, sort=True, add_na=False, strict=False):
        if hasattr(col, 'dtype') and isinstance(col.dtype, CategoricalDtype):
            items = L(col.cat.categories, use_list=True)
            #Remove non-used categories while keeping order
            if strict: items = L(o for o in items if o in col.unique())
        else:
            if not hasattr(col,'unique'): col = L(col, use_list=True)
            # `o==o` is the generalized definition of non-NaN used by Pandas
            items = col.unique()
            # Dask compatibility
            if hasattr(items, "compute"):
                items = items.compute()
                # Dask sometimes (always?) represents NANs as `pandas._libs.missing.NAType` values 
                # which do not work with the `o==o` condition (TypeError: boolean value of NA is ambiguous)
                items = items.dropna()
            items = L(o for o in items if o==o)
            if sort: items = items.sorted()
        self.items = '#na#' + items if add_na else items
        self.o2i = defaultdict(int, self.items.val2idx()) if add_na else dict(self.items.val2idx())

# %% ../nbs/00_core.ipynb 43
class DaskCategorify(TabularProc):
    "Transform the categorical variables to something similar to `pd.Categorical`"
    order = 1
    def __init__(self, cat_vocabs:'dict | None'=None):
        # self.classes = {}
        # if cat_vocabs is not None:
        #     self.classes = {n: DaskCategoryMap(vocab, sort=False, add_na=False) for n, vocab in cat_vocabs.items()}
        classes = {}
        if cat_vocabs is not None:
            classes = {n: DaskCategoryMap(vocab, sort=False, add_na=False) for n, vocab in cat_vocabs.items()}
        store_attr(classes=classes, but='to')

    def setups(self, to):
        for n in to.cat_names:
            if n not in self.classes:
                self.classes[n] = DaskCategoryMap(getattr(to, 'train', to).iloc[:,n], add_na=(n in to.cat_names))
        # store_attr(classes={n:DaskCategoryMap(to.iloc[:,n], add_na=(n in to.cat_names)) for n in to.cat_names}, but='to')
        # self(to)

    def encodes(self, to): to.transform(list(self.classes.keys()), partial(_apply_cats, voc=self.classes, add=1))
    def decodes(self, to): to.transform(list(self.classes.keys()), partial(_decode_cats, voc=self.classes))
    def __getitem__(self,k): return self.classes[k]

# %% ../nbs/00_core.ipynb 48
def _apply_cats (c, voc, add):
    if not (hasattr(c, 'dtype') and isinstance(c.dtype, CategoricalDtype)):
        return pd.Categorical(c, categories=voc[c.name][add:]).codes+add
    return c.cat.codes+add #if is_categorical_dtype(c) else c.map(voc[c.name].o2i)
def _decode_cats(c, voc): return c.map(dict(enumerate(voc[c.name].items)))

# %% ../nbs/00_core.ipynb 65
class DaskNormalize(TabularProc):
    parameters,order = L('mean', 'std'),99
    def __init__(self, cols=None):
        self.cols = listify(cols)

    def setups(self, to):
        if not self.cols: self.cols = listify(to.cont_names)
        # store_attr(but='to', means=getattr(to, 'train', to).conts.mean().compute(),
        #            stds=getattr(to, 'train', to).conts.std(ddof=0).compute()+1e-7)
        store_attr(but='to', means=getattr(to, 'train', to).items[self.cols].mean().compute(),
                   stds=getattr(to, 'train', to).items[self.cols].std(ddof=0).compute())
        return self(to)

    def encodes(self, to):
        # to.conts = to.conts.map_partitions(lambda df: (df-self.means) / self.stds)
        to.items[self.cols] = to.items[self.cols].map_partitions(lambda df: (df-self.means) / self.stds)
        return to

    def decodes(self, to):
        # to.conts = to.conts.map_partitions(lambda df: (df*self.stds) + self.means)
        to.items[self.cols] = to.items[self.cols].map_partitions(lambda df: (df*self.stds) + self.means)
        return to

# %% ../nbs/00_core.ipynb 66
class DaskCategorize(DisplayedTransform):
    loss_func,order=CrossEntropyLossFlat(),1
    def __init__(self, vocab=None, sort=True, add_na=False):
        self.vocab = vocab
        if vocab is not None: self.vocab = DaskCategoryMap(vocab, sort=sort, add_na=add_na)

    def setups(self, to):
        if len(to.y_names) > 0:
            if self.vocab is None:
                self.vocab = DaskCategoryMap(getattr(to, 'train', to).iloc[:,to.y_names[0]], strict=True)
            else:
                self.vocab = DaskCategoryMap(self.vocab, sort=False, add_na=self.add_na)
            self.c = len(self.vocab)
        return self(to)

    def encodes(self, to):
        to.transform(to.y_names, partial(_apply_cats, voc={n: self.vocab for n in to.y_names}, add=0), all_col=False)
        return to

    def decodes(self, to):
        to.transform(to.y_names, partial(_decode_cats, voc={n: self.vocab for n in to.y_names}), all_col=False)
        return to

# %% ../nbs/00_core.ipynb 67
class DaskFillStrategy:
    "Namespace containing the various filling strategies."
    def median  (c,fill): return c.median_approximate().compute()
    def constant(c,fill): return fill
    def mode    (c,fill): return c.dropna().value_counts().idxmax().compute()

# %% ../nbs/00_core.ipynb 69
class DaskFillMissing(TabularProc):
    "Fill the missing values in continuous columns."
    def __init__(self, fill_strategy=DaskFillStrategy.median, add_col=True, fill_vals=None):
        if fill_vals is None: fill_vals = defaultdict(int)
        store_attr()

    def setups(self, to):
        missing = to.conts.isnull().any().compute()
        store_attr(but='to', na_dict={n:self.fill_strategy(to[n], self.fill_vals[n])
                            for n in missing[missing].keys()})
        self.fill_strategy = self.fill_strategy.__name__
        # return self(to)

    def encodes(self, to):
        missing = to.conts.isnull()
        missing_any = missing.any().compute()
        for n in missing_any[missing_any].keys():
            assert n in self.na_dict, f"nan values in `{n}` but not in setup training set"
        if self.na_dict:
            to.items = to.items.fillna(self.na_dict)
            if self.add_col:
                for n in self.na_dict.keys():
                    to.items[n+'_na'] = missing[n]
                    if n+'_na' not in to.cat_names: to.cat_names.append(n+'_na')

# %% ../nbs/00_core.ipynb 71
class DaskRegressionSetup(DisplayedTransform):
    "A Dask-compatible transform that floatifies targets"
    loss_func=MSELossFlat()
    def __init__(self, c=None): store_attr()

    def setups(self, to):
        if self.c is not None: return
        self.c = len(to.y_names)
        return self(to)

    def encodes(self, to):
        for c in to.y_names:
            if c in to.items.columns: to[c] = to[c].astype("float")
        return to
    def decodes(self, to): return to

# %% ../nbs/00_core.ipynb 73
def DaskCategoryBlock(
    vocab:MutableSequence|pd.Series=None, # List of unique class names
    sort:bool=True, # Sort the classes alphabetically
    add_na:bool=False, # Add `#na#` to `vocab`
):
    "A Dask-compatible `TransformBlock` for single-label categorical targets"
    return TransformBlock(type_tfms=DaskCategorize(vocab=vocab, sort=sort, add_na=add_na))

# %% ../nbs/00_core.ipynb 74
def DaskRegressionBlock(
    n_out:int=None, # Number of output values
):
    "A Dask-compatible `TransformBlock` for float targets"
    return TransformBlock(type_tfms=DaskRegressionSetup(c=n_out))

# %% ../nbs/00_core.ipynb 91
class DaskDataLoader(DataLoader):
    "Iterable dataloader for tabular learning with Dask"
    # TODO: align with TabDataLoader + ReadTabBatch (fastai.tabular.core)?
    def create_batch(self, b):
        b = list(map(np.array, zip(*b)))
        cats, conts = tensor(b[0]).long(), tensor(b[1]).float()
        res = (cats, conts)
        # add target if available
        if len(b) > 2: res = res + (tensor(b[2]),)
        return res

    def decode(self, b):
        tmp = self.dataset.new(dd.from_pandas(b))
        return tmp.decode().items.compute().drop(columns="_int_train_mask")

    def show_batch(self,
        b=None, # Batch to show
        max_n:int=9, # Maximum number of items to show,
        show:bool=True, # Whether to display data
    ):
        "Show `max_n` input(s) and target(s) from the batch."
        if b is None: b = self.one_batch()
        x1 = pd.DataFrame(b[0][:max_n].cpu().numpy(), columns=self.dataset.cat_names)
        x2 = pd.DataFrame(b[1][:max_n].cpu().numpy(), columns=self.dataset.cont_names)
        b_ = [x1, x2]
        if len(b) > 2:
            y = pd.DataFrame(b[2][:max_n].cpu().numpy(), columns=self.dataset.y_names)
            b_.append(y)
        b = pd.concat(b_, axis=1)
        if not show: return b
        b = self.decode(b)
        display_df(b)

    def show_results(self, 
        b, # Batch to show results for
        out, # Predicted output from model for the batch
        max_n:int=9, # Maximum number of items to show
        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc
        show:bool=True, # Whether to display data
        **kwargs
    ):
        "Show `max_n` results with input(s), target(s) and prediction(s)."
        df = self.show_batch(b, max_n=max_n, show=False)
        yhat = pd.DataFrame(out[:max_n].cpu().numpy(), columns=[n+'_pred' for n in self.dataset.y_names])
        if show: display_df(pd.concat([df, yhat], axis=1))

TabularDask._dl_type = DaskDataLoader
TabularDask._dbunch_type = DataLoaders

# %% ../nbs/00_core.ipynb 110
@EncodedMultiCategorize
def setups(self, to:Tabular):
    self.c = len(self.vocab)
    return self(to)

@EncodedMultiCategorize
def encodes(self, to:TabularDask): return to

@EncodedMultiCategorize
def decodes(self, to:TabularDask):
    to.transform(to.y_names, lambda c: c==1)
    return to
